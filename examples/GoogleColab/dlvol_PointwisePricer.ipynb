{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Random Grids Pointwise Network Training for Google Colab\n",
        "=========================================================\n",
        "Complete script for training pointwise networks using Random Grids/Random Smiles\n",
        "based on the approach by Baschetti et al.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================================================\n",
        "# === CELL 1: SETUP AND INSTALLATION ===\n",
        "# ===============================================================\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# 1. Enter token\n",
        "token = getpass(\"Enter your GitHub PAT here (scope=repo): \")\n",
        "\n",
        "# 2. Define project path and branch name\n",
        "project_dir = \"/content/deepLearningVolatility\"\n",
        "branch_name = \"master\"\n",
        "\n",
        "# 3. Clean up residuals and clone the repository\n",
        "if os.path.exists(project_dir):\n",
        "    !rm -rf {project_dir}\n",
        "!git clone --depth 1 --branch {branch_name} https://{token}@github.com/BianchiGiacomo/deepLearningVolatility.git {project_dir}\n",
        "\n",
        "# 4. Install dependencies with Poetry\n",
        "print(\"\\n--> Installing project dependencies with Poetry...\")\n",
        "!cd {project_dir} && pip install poetry && poetry install\n",
        "\n",
        "# 5. Verify installation\n",
        "import importlib.util\n",
        "spec = importlib.util.find_spec(\"deepLearningVolatility\")\n",
        "\n",
        "if spec:\n",
        "    print(\"\\n✓ Installation completed successfully!\")\n",
        "    print(f\" The package 'deepLearningVolatility' is now installed.\")\n",
        "else:\n",
        "    print(\"\\nX ERROR: Installation seems to have failed.\")\n",
        "\n",
        "# Set working directory\n",
        "os.chdir(project_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbDF3Jyb66Sg",
        "outputId": "976e98f7-b8a6-4c3b-e1d3-769158da5f62"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserisci qui il tuo PAT GitHub (scope=repo): ··········\n",
            "Cloning into '/content/deepLearningVolatility'...\n",
            "remote: Enumerating objects: 125, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "remote: Total 125 (delta 18), reused 96 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (125/125), 2.28 MiB | 15.09 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "\n",
            "--> Installing project dependencies with Poetry...\n",
            "Collecting poetry\n",
            "  Downloading poetry-2.1.4-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: build<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from poetry) (1.3.0)\n",
            "Requirement already satisfied: cachecontrol<0.15.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (0.14.3)\n",
            "Collecting cleo<3.0.0,>=2.1.0 (from poetry)\n",
            "  Downloading cleo-2.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dulwich<0.23.0,>=0.22.6 (from poetry)\n",
            "  Downloading dulwich-0.22.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: fastjsonschema<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (2.21.2)\n",
            "Collecting findpython<0.7.0,>=0.6.2 (from poetry)\n",
            "  Downloading findpython-0.6.3-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting installer<0.8.0,>=0.7.0 (from poetry)\n",
            "  Downloading installer-0.7.0-py3-none-any.whl.metadata (936 bytes)\n",
            "Requirement already satisfied: keyring<26.0.0,>=25.1.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (25.6.0)\n",
            "Requirement already satisfied: packaging>=24.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (25.0)\n",
            "Collecting pbs-installer<2026.0.0,>=2025.1.6 (from pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry)\n",
            "  Downloading pbs_installer-2025.8.28-py3-none-any.whl.metadata (991 bytes)\n",
            "Collecting pkginfo<2.0,>=1.12 (from poetry)\n",
            "  Downloading pkginfo-1.12.1.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (4.3.8)\n",
            "Collecting poetry-core==2.1.3 (from poetry)\n",
            "  Downloading poetry_core-2.1.3-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (1.2.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.26 in /usr/local/lib/python3.12/dist-packages (from poetry) (2.32.4)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (1.0.0)\n",
            "Requirement already satisfied: shellingham<2.0,>=1.5 in /usr/local/lib/python3.12/dist-packages (from poetry) (1.5.4)\n",
            "Requirement already satisfied: tomlkit<1.0.0,>=0.11.4 in /usr/local/lib/python3.12/dist-packages (from poetry) (0.13.3)\n",
            "Collecting trove-classifiers>=2022.5.19 (from poetry)\n",
            "  Downloading trove_classifiers-2025.8.26.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting virtualenv<20.33.0,>=20.26.6 (from poetry)\n",
            "  Downloading virtualenv-20.32.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.12/dist-packages (from cachecontrol<0.15.0,>=0.14.0->cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (1.1.1)\n",
            "Requirement already satisfied: filelock>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (3.19.1)\n",
            "Collecting crashtest<0.5.0,>=0.4.1 (from cleo<3.0.0,>=2.1.0->poetry)\n",
            "  Downloading crashtest-0.4.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.0.0 (from cleo<3.0.0,>=2.1.0->poetry)\n",
            "  Downloading rapidfuzz-3.14.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from dulwich<0.23.0,>=0.22.6->poetry) (2.5.0)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (3.3.3)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (0.9.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (3.4.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (4.3.0)\n",
            "Requirement already satisfied: jaraco.context in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (6.0.1)\n",
            "Requirement already satisfied: httpx<1,>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (0.28.1)\n",
            "Requirement already satisfied: zstandard>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (0.24.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.26->poetry) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.26->poetry) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.26->poetry) (2025.8.3)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv<20.33.0,>=20.26.6->poetry)\n",
            "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (0.16.0)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.12/dist-packages (from SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (43.0.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from jaraco.classes->keyring<26.0.0,>=25.1.0->poetry) (10.7.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (1.17.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (2.22)\n",
            "Downloading poetry-2.1.4-py3-none-any.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.7/278.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading poetry_core-2.1.3-py3-none-any.whl (332 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.6/332.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cleo-2.1.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dulwich-0.22.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading findpython-0.6.3-py3-none-any.whl (20 kB)\n",
            "Downloading installer-0.7.0-py3-none-any.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.8/453.8 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pbs_installer-2025.8.28-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pkginfo-1.12.1.2-py3-none-any.whl (32 kB)\n",
            "Downloading trove_classifiers-2025.8.26.11-py3-none-any.whl (14 kB)\n",
            "Downloading virtualenv-20.32.0-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crashtest-0.4.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trove-classifiers, distlib, virtualenv, rapidfuzz, poetry-core, pkginfo, pbs-installer, installer, findpython, dulwich, crashtest, cleo, poetry\n",
            "Successfully installed cleo-2.1.0 crashtest-0.4.1 distlib-0.4.0 dulwich-0.22.8 findpython-0.6.3 installer-0.7.0 pbs-installer-2025.8.28 pkginfo-1.12.1.2 poetry-2.1.4 poetry-core-2.1.3 rapidfuzz-3.14.0 trove-classifiers-2025.8.26.11 virtualenv-20.32.0\n",
            "\u001b[33mThe \"poetry.dev-dependencies\" section is deprecated and will be removed in a future version. Use \"poetry.group.dev.dependencies\" instead.\u001b[39m\n",
            "Creating virtualenv \u001b[36mdeep-learning-volatility-KvqlgsK9-py3.12\u001b[39m in /root/.cache/pypoetry/virtualenvs\n",
            "\u001b[34mUpdating dependencies\u001b[39m\n",
            "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(8.1s)\u001b[39;22m\u001b[33mThe \"poetry.dev-dependencies\" section is deprecated and will be removed in a future version. Use \"poetry.group.dev.dependencies\" instead.\u001b[39m\n",
            "\u001b[2K\n",
            "\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(23.3s)\u001b[39;22m\n",
            "\u001b[31;1mThe current project's supported Python range (>=3.8.1,<3.9) is not compatible with some of the required packages Python requirement:\n",
            "  - matplotlib requires Python >=3.10, so it will not be installable for Python >=3.8.1,<3.9\n",
            "  - matplotlib requires Python >=3.10, so it will not be installable for Python >=3.8.1,<3.9\n",
            "  - matplotlib requires Python >=3.10, so it will not be installable for Python >=3.8.1,<3.9\n",
            "  - matplotlib requires Python >=3.9, so it will not be installable for Python >=3.8.1,<3.9\n",
            "  - matplotlib requires Python >=3.9, so it will not be installable for Python >=3.8.1,<3.9\n",
            "  - matplotlib requires Python >=3.9, so it will not be installable for Python >=3.8.1,<3.9\n",
            "  - matplotlib requires Python >=3.9, so it will not be installable for Python >=3.8.1,<3.9\n",
            "  - matplotlib requires Python >=3.9, so it will not be installable for Python >=3.8.1,<3.9\n",
            "  - matplotlib requires Python >=3.9, so it will not be installable for Python >=3.8.1,<3.9\n",
            "  - matplotlib requires Python >=3.9, so it will not be installable for Python >=3.8.1,<3.9\n",
            "  - matplotlib requires Python >=3.9, so it will not be installable for Python >=3.8.1,<3.9\n",
            "  - matplotlib requires Python >=3.9, so it will not be installable for Python >=3.8.1,<3.9\n",
            "  - matplotlib requires Python >=3.9, so it will not be installable for Python >=3.8.1,<3.9\n",
            "  - matplotlib requires Python >=3.10, so it will not be installable for Python >=3.8.1,<3.9\n",
            "  - matplotlib requires Python >=3.10, so it will not be installable for Python >=3.8.1,<3.9\n",
            "\n",
            "Because no versions of matplotlib match >3.8.0,<3.8.1 || >3.8.1,<3.8.2 || >3.8.2,<3.8.3 || >3.8.3,<3.8.4 || >3.8.4,<3.9.0 || >3.9.0,<3.9.1.post1 || >3.9.1.post1,<3.9.2 || >3.9.2,<3.9.3 || >3.9.3,<3.9.4 || >3.9.4,<3.10.0 || >3.10.0,<3.10.1 || >3.10.1,<3.10.3 || >3.10.3,<3.10.5 || >3.10.5,<3.10.6 || >3.10.6,<4.0.0\n",
            " and matplotlib (3.8.0) requires Python >=3.9, matplotlib is forbidden.\n",
            "And because matplotlib (3.8.1) requires Python >=3.9, matplotlib is forbidden.\n",
            "And because matplotlib (3.8.2) requires Python >=3.9\n",
            " and matplotlib (3.8.3) requires Python >=3.9, matplotlib is forbidden.\n",
            "And because matplotlib (3.8.4) requires Python >=3.9\n",
            " and matplotlib (3.9.0) requires Python >=3.9, matplotlib is forbidden.\n",
            "And because matplotlib (3.9.1.post1) requires Python >=3.9\n",
            " and matplotlib (3.9.2) requires Python >=3.9, matplotlib is forbidden.\n",
            "And because matplotlib (3.9.3) requires Python >=3.9\n",
            " and matplotlib (3.9.4) requires Python >=3.9, matplotlib is forbidden.\n",
            "And because matplotlib (3.10.0) requires Python >=3.10\n",
            " and matplotlib (3.10.1) requires Python >=3.10, matplotlib is forbidden.\n",
            "And because matplotlib (3.10.3) requires Python >=3.10\n",
            " and matplotlib (3.10.5) requires Python >=3.10, matplotlib is forbidden.\n",
            "So, because matplotlib (3.10.6) requires Python >=3.10\n",
            " and deep-learning-volatility depends on matplotlib (^3.8.0), version solving failed.\n",
            "\n",
            "  \u001b[39;22m\u001b[34;1m* \u001b[39;22m\u001b[39;1mCheck your dependencies Python requirement\u001b[39;22m\u001b[31;1m: The Python requirement can be specified via the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` or `\u001b[39;22m\u001b[39;1mmarkers\u001b[39;22m\u001b[31;1m` properties\n",
            "\n",
            "    For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "For \u001b[39;22m\u001b[39;1mmatplotlib\u001b[39;22m\u001b[31;1m, a possible solution would be to set the `\u001b[39;22m\u001b[39;1mpython\u001b[39;22m\u001b[31;1m` property to \u001b[39;22m\u001b[33m\"\u001b[39m\u001b[33m<empty>\u001b[39m\u001b[33m\"\u001b[39m\u001b[31;1m\n",
            "\n",
            "    \u001b[39;22m\u001b[34mhttps://python-poetry.org/docs/dependency-specification/#python-restricted-dependencies\u001b[39m\u001b[31;1m,\n",
            "    \u001b[39;22m\u001b[34mhttps://python-poetry.org/docs/dependency-specification/#using-environment-markers\u001b[39m\u001b[31;1m\n",
            "\u001b[39;22m\n",
            "\n",
            "✓ Installazione completata con successo!\n",
            " Il pacchetto 'deepLearningVolatility' è ora installato.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# === CELL 2: IMPORTS AND CONFIGURATION ===\n",
        "# ===============================================================\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import json\n",
        "import gc\n",
        "import time\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Import from project libraries\n",
        "from deepLearningVolatility.stochastic.stochastic_interface import ProcessFactory\n",
        "from deepLearningVolatility.nn.dataset_builder import DatasetBuilder\n",
        "from deepLearningVolatility.nn.pricer.pricer import PointwiseNetworkPricer\n",
        "\n",
        "# Import wrappers to register them\n",
        "import deepLearningVolatility.stochastic.wrappers.heston_wrapper\n",
        "import deepLearningVolatility.stochastic.wrappers.rough_heston_wrapper\n",
        "import deepLearningVolatility.stochastic.wrappers.rough_bergomi_wrapper\n",
        "\n",
        "print(\"✔ Class imports successful!\")\n",
        "print(\"GPU available?\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Device:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB1ggddQ66lo",
        "outputId": "85e01190-0011-4e5d-e67e-0488eaba6f12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/deepLearningVolatility/deepLearningVolatility/nn/functional.py:291: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  If :math:`p \\leq 1 / N`` with :math:`N` being the number of elements to sort,\n",
            "/content/deepLearningVolatility/deepLearningVolatility/nn/functional.py:332: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  for :math:`\\lambda\\geq1`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Import delle classi riuscito!\n",
            "GPU disponibile? True\n",
            "Device: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# === CELL 3: CONFIGURATION AND GOOGLE DRIVE SETUP ===\n",
        "# ===============================================================\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Main configuration\n",
        "CONFIG = {\n",
        "    # Stochastic process\n",
        "    \"process\": \"rough_bergomi\",  # Options: \"heston\", \"rough_heston\", \"rough_bergomi\"\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "\n",
        "    # Dataset parameters - Random Grids\n",
        "    \"train_surfaces\": 7_000,      # Number of surfaces for training\n",
        "    \"train_maturities\": 11,       # Number of maturities per surface\n",
        "    \"train_strikes\": 13,          # Number of strikes per maturity\n",
        "\n",
        "    # Dataset parameters - Random Smiles for validation\n",
        "    \"val_smiles\": 10_000,           # Number of smiles for validation\n",
        "    \"val_strikes\": 13,            # Strikes per smile\n",
        "\n",
        "    # Monte Carlo parameters\n",
        "    \"mc_paths\": 50_000,            # Base number of MC paths\n",
        "    \"spot\": 1.0,\n",
        "    \"seed\": 42,\n",
        "\n",
        "    # Training parameters\n",
        "    \"epochs\": 100,\n",
        "    \"batch_size\": 4096,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"lr_scheduler\": True,\n",
        "    \"early_stopping_patience\": 20,\n",
        "\n",
        "    # Checkpoint settings\n",
        "    \"dataset_checkpoint_every\": 500,  # Save every 100 surfaces\n",
        "    \"resume_dataset_generation\": True,  # Resume from checkpoint if available\n",
        "    \"cleanup_checkpoints_after_completion\": False,\n",
        "\n",
        "    # Network architecture\n",
        "    \"hidden_layers\": [30, 30, 30, 30],\n",
        "    \"activation\": \"ELU\",\n",
        "\n",
        "    # Training resume / warm-start\n",
        "    \"resume_training\": True,                 # True to resume from checkpoint\n",
        "    \"resume_from\": \"latest\",                  # \"latest\" or path to the .pt checkpoint\n",
        "    \"warm_start\": True,                      # True to start from saved weights\n",
        "    \"warm_start_path\": None,                  # if None uses best_model.pt or latest\n",
        "\n",
        "    # Output directories\n",
        "    \"output_dir\": \"/content/drive/MyDrive/random_grids_results\",\n",
        "}\n",
        "\n",
        "# Create directory structure on Drive\n",
        "output_dir = CONFIG[\"output_dir\"]\n",
        "os.makedirs(f\"{output_dir}/models\", exist_ok=True)\n",
        "os.makedirs(f\"{output_dir}/models/checkpoints\", exist_ok=True)\n",
        "os.makedirs(f\"{output_dir}/models/final\", exist_ok=True)\n",
        "os.makedirs(f\"{output_dir}/datasets\", exist_ok=True)\n",
        "os.makedirs(f\"{output_dir}/logs\", exist_ok=True)\n",
        "os.makedirs(f\"{output_dir}/visualizations\", exist_ok=True)\n",
        "\n",
        "print(f\"\\n✔ Directories created in: {output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ5R9STS66sX",
        "outputId": "39789f12-dee9-4c87-9afd-8b44366f4bb4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "✔ Directory create in: /content/drive/MyDrive/random_grids_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# === CELL 4: UTILITY FUNCTIONS ===\n",
        "# ===============================================================\n",
        "\n",
        "def print_dataset_stats(theta, T, k, iv, name=\"Dataset\"):\n",
        "    \"\"\"Prints dataset statistics\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{name} Statistics\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    def _stats(x, label):\n",
        "        return (f\"{label:<10}: shape={str(tuple(x.shape)):<20} \"\n",
        "                f\"mean={x.mean().item():.4f} std={x.std().item():.4f} \"\n",
        "                f\"range=[{x.min().item():.4f}, {x.max().item():.4f}]\")\n",
        "\n",
        "    print(_stats(theta, \"Theta\"))\n",
        "    print(_stats(T, \"T\"))\n",
        "    print(_stats(k, \"k\"))\n",
        "    print(_stats(iv, \"IV\"))\n",
        "    print(f\"Total points: {len(theta):,}\")\n",
        "\n",
        "\n",
        "def visualize_random_grids_sample(builder, theta, T, k, iv, n_samples=2):\n",
        "    \"\"\"Visualizes some examples of generated random grids\"\"\"\n",
        "    fig, axes = plt.subplots(n_samples, 3, figsize=(15, 5*n_samples))\n",
        "\n",
        "    if n_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    # Denormalize for visualization\n",
        "    theta_denorm = builder.denormalize_theta(theta)\n",
        "    T_denorm = builder.denormalize_T(T) if builder.T_mean is not None else T\n",
        "    k_denorm = builder.denormalize_k(k) if builder.k_mean is not None else k\n",
        "    iv_denorm = builder.denormalize_iv(iv)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Find points for this theta\n",
        "        theta_idx = i\n",
        "        mask = torch.all(theta == theta[theta_idx], dim=1).cpu().numpy()\n",
        "\n",
        "        T_sample = T_denorm[mask].cpu().numpy()\n",
        "        k_sample = k_denorm[mask].cpu().numpy()\n",
        "        iv_sample = iv_denorm[mask].cpu().numpy()\n",
        "\n",
        "        # Scatter plot of the grid\n",
        "        scatter = axes[i, 0].scatter(k_sample, T_sample, c=iv_sample,\n",
        "                                     cmap='viridis', s=20)\n",
        "        axes[i, 0].set_xlabel('Log-Moneyness')\n",
        "        axes[i, 0].set_ylabel('Maturity')\n",
        "        axes[i, 0].set_title(f'Sample {i+1}: Random Grid Points')\n",
        "        plt.colorbar(scatter, ax=axes[i, 0], label='IV')\n",
        "\n",
        "        # Distribution of maturities\n",
        "        unique_T = np.unique(T_sample)\n",
        "        axes[i, 1].hist(T_sample, bins=len(unique_T), alpha=0.7, edgecolor='black')\n",
        "        axes[i, 1].set_xlabel('Maturity')\n",
        "        axes[i, 1].set_ylabel('Count')\n",
        "        axes[i, 1].set_title(f'Maturity Distribution ({len(unique_T)} unique)')\n",
        "\n",
        "        # Approximate IV surface heatmap\n",
        "        from scipy.interpolate import griddata\n",
        "        Ti = np.linspace(T_sample.min(), T_sample.max(), 50)\n",
        "        ki = np.linspace(k_sample.min(), k_sample.max(), 50)\n",
        "        Ti_grid, ki_grid = np.meshgrid(Ti, ki)\n",
        "\n",
        "        iv_interp = griddata((T_sample, k_sample), iv_sample,\n",
        "                            (Ti_grid, ki_grid), method='linear')\n",
        "\n",
        "        im = axes[i, 2].imshow(iv_interp.T, extent=[ki.min(), ki.max(),\n",
        "                                                     Ti.min(), Ti.max()],\n",
        "                               aspect='auto', origin='lower', cmap='viridis')\n",
        "        axes[i, 2].set_xlabel('Log-Moneyness')\n",
        "        axes[i, 2].set_ylabel('Maturity')\n",
        "        axes[i, 2].set_title('Interpolated Surface')\n",
        "        plt.colorbar(im, ax=axes[i, 2], label='IV')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{CONFIG['output_dir']}/visualizations/random_grids_sample.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_training_history(train_losses, val_losses, save_path=None):\n",
        "    \"\"\"Visualizes training curves\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Full loss\n",
        "    ax1.plot(train_losses, label='Train Loss', linewidth=2)\n",
        "    ax1.plot(val_losses, label='Val Loss', linewidth=2)\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('MSE Loss')\n",
        "    ax1.set_title('Training History')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_yscale('log')\n",
        "\n",
        "    # Zoom on last epochs\n",
        "    zoom_start = max(0, len(train_losses) - 30)\n",
        "    ax2.plot(range(zoom_start, len(train_losses)),\n",
        "             train_losses[zoom_start:], label='Train Loss', linewidth=2)\n",
        "    ax2.plot(range(zoom_start, len(val_losses)),\n",
        "             val_losses[zoom_start:], label='Val Loss', linewidth=2)\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('MSE Loss')\n",
        "    ax2.set_title('Last 30 Epochs')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "def cleanup_dataset_checkpoints(checkpoint_dir):\n",
        "    \"\"\"Removes dataset checkpoints after completion.\"\"\"\n",
        "    import shutil\n",
        "    if os.path.exists(checkpoint_dir):\n",
        "        print(f\"Cleaning up checkpoints in {checkpoint_dir} ...\")\n",
        "        shutil.rmtree(checkpoint_dir)\n",
        "        print(\"✓ Checkpoints removed\")\n",
        "\n",
        "def test_predictions(pricer, builder, theta_test, T_test, k_test, iv_test, n_samples=5):\n",
        "    \"\"\"Tests model predictions\"\"\"\n",
        "    pricer.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Predictions (output already denormalized by price_iv)\n",
        "        iv_pred = pricer.price_iv(theta_test, T_test, k_test, denormalize_output=True, inputs_normalized=True)\n",
        "\n",
        "        # Denormalize true values\n",
        "        iv_true = builder.denormalize_iv(iv_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        mae = (iv_pred - iv_true).abs().mean()\n",
        "        rmse = ((iv_pred - iv_true) ** 2).mean().sqrt()\n",
        "        mape = ((iv_pred - iv_true).abs() / (iv_true + 1e-8)).mean() * 100\n",
        "\n",
        "        print(f\"\\nTest Metrics:\")\n",
        "        print(f\"  MAE:  {mae:.6f}\")\n",
        "        print(f\"  RMSE: {rmse:.6f}\")\n",
        "        print(f\"  MAPE: {mape:.2f}%\")\n",
        "\n",
        "        # Visualize some examples\n",
        "        fig, axes = plt.subplots(1, n_samples, figsize=(4*n_samples, 4))\n",
        "        if n_samples == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i in range(min(n_samples, len(theta_test))):\n",
        "            # Find all points for this theta\n",
        "            theta_i = theta_test[i:i+1]\n",
        "            mask = torch.all(theta_test == theta_i, dim=1)\n",
        "\n",
        "            T_i = builder.denormalize_T(T_test[mask]) if builder.T_mean is not None else T_test[mask]\n",
        "            k_i = builder.denormalize_k(k_test[mask]) if builder.k_mean is not None else k_test[mask]\n",
        "            iv_true_i = iv_true[mask]\n",
        "            iv_pred_i = iv_pred[mask]\n",
        "\n",
        "            # Sort by T then k for visualization\n",
        "            sort_idx = torch.argsort(T_i * 1000 + k_i)\n",
        "\n",
        "            axes[i].scatter(k_i[sort_idx].cpu(), iv_true_i[sort_idx].cpu(),\n",
        "                          alpha=0.6, label='True', s=20)\n",
        "            axes[i].scatter(k_i[sort_idx].cpu(), iv_pred_i[sort_idx].cpu(),\n",
        "                          alpha=0.6, label='Predicted', s=20)\n",
        "            axes[i].set_xlabel('Log-Moneyness')\n",
        "            axes[i].set_ylabel('IV')\n",
        "            axes[i].set_title(f'Sample {i+1}')\n",
        "            axes[i].legend()\n",
        "            axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{CONFIG['output_dir']}/visualizations/predictions_comparison.png\", dpi=150)\n",
        "        plt.show()\n",
        "\n",
        "    return mae, rmse, mape\n",
        "\n",
        "def generate_mc_reference_surface(builder, theta, maturities, logK, n_paths=50000):\n",
        "    \"\"\"\n",
        "    Generates reference surface via Monte Carlo for comparison\n",
        "\n",
        "    Args:\n",
        "        builder: DatasetBuilder with the process\n",
        "        theta: Model parameters (not normalized)\n",
        "        maturities: Array of maturities\n",
        "        logK: Array of log-moneyness\n",
        "        n_paths: Number of MC paths for high precision\n",
        "    \"\"\"\n",
        "    print(f\"Generating MC reference surface with {n_paths} paths...\")\n",
        "\n",
        "    # Create a temporary pricer for MC\n",
        "    from deepLearningVolatility.nn.pricer.pricer import GridNetworkPricer\n",
        "\n",
        "    maturities = maturities.to(builder.device).float().contiguous()\n",
        "    logK = logK.to(builder.device).float().contiguous()\n",
        "    theta = theta.to(builder.device).float().contiguous()\n",
        "\n",
        "    temp_pricer = GridNetworkPricer(\n",
        "        maturities=maturities,\n",
        "        logK=logK,\n",
        "        process=builder.process,\n",
        "        device=builder.device,\n",
        "        enable_smile_repair=True,\n",
        "        smile_repair_method='pchip'\n",
        "    )\n",
        "\n",
        "    # Generate surface via MC\n",
        "    with torch.no_grad():\n",
        "        iv_mc = temp_pricer._mc_iv_grid(\n",
        "            theta=theta,\n",
        "            n_paths=n_paths,\n",
        "            spot=1.0,\n",
        "            use_antithetic=True,\n",
        "            adaptive_dt=True,\n",
        "            control_variate=True\n",
        "        )\n",
        "\n",
        "    return iv_mc\n",
        "\n",
        "def visualize_mc_vs_nn_comparison(builder, pricer, theta, maturities, logK,\n",
        "                                  n_mc_paths=50000, save_path=None):\n",
        "    \"\"\"\n",
        "    Compares MC vs NN using, for each T, an **in-domain** k-grid (as in training).\n",
        "    Note: `len(logK)` only determines the number of points per smile; k values\n",
        "    are recalculated for each T based on √T.\n",
        "    \"\"\"\n",
        "    n_T = len(maturities)\n",
        "    n_K = len(logK)  # only the NUMBER of points per smile\n",
        "\n",
        "    # Strike domain parameters used in training (fallback to defaults)\n",
        "    sp = getattr(builder, \"_strike_params\", {\"l\": 0.55, \"u\": 0.30})\n",
        "    l, u = float(sp[\"l\"]), float(sp[\"u\"])\n",
        "    spot = 1.0\n",
        "    safety_eps = 1e-6\n",
        "\n",
        "    # Preallocate\n",
        "    k_mat  = torch.empty((n_T, n_K), device=builder.device, dtype=torch.float32)\n",
        "    iv_mc  = torch.empty((n_T, n_K), device=builder.device, dtype=torch.float32)\n",
        "    iv_nn  = torch.empty((n_T, n_K), device=builder.device, dtype=torch.float32)\n",
        "\n",
        "    # Per-T calculation (no extrapolation)\n",
        "    pricer.eval()\n",
        "    with torch.no_grad():\n",
        "        for j, T in enumerate(maturities):\n",
        "            T_val = float(T.item())\n",
        "            sqrtT = T_val ** 0.5\n",
        "            K_min = max(spot * (1.0 - l * sqrtT), 0.05 * spot)\n",
        "            K_max = min(spot * (1.0 + u * sqrtT), 3.00 * spot)\n",
        "            k_min = math.log(K_min / spot) + safety_eps\n",
        "            k_max = math.log(K_max / spot) - safety_eps\n",
        "            k_vec = torch.linspace(k_min, k_max, n_K, device=builder.device)\n",
        "            k_mat[j, :] = k_vec\n",
        "\n",
        "            # MC on (T, k_vec)\n",
        "            iv_mc_j = pricer._mc_iv_grid(\n",
        "                theta=theta,\n",
        "                maturities=T.view(1).to(builder.device),\n",
        "                logK=k_vec,\n",
        "                n_paths=n_mc_paths,\n",
        "                spot=spot,\n",
        "                use_antithetic=True,\n",
        "                control_variate=True\n",
        "            ).squeeze(0)\n",
        "            iv_mc[j, :] = iv_mc_j\n",
        "\n",
        "            # NN on the same grid (RAW input → default inputs_normalized=False)\n",
        "            iv_nn_j = pricer.price_iv_grid(\n",
        "                theta=theta,\n",
        "                maturities=T.view(1).to(builder.device),\n",
        "                logK=k_vec,\n",
        "                denormalize_output=True\n",
        "            ).squeeze(0)\n",
        "            iv_nn[j, :] = iv_nn_j\n",
        "\n",
        "\n",
        "    # Figure for smile comparison (max 5 maturities)\n",
        "    n_T_to_plot = min(5, n_T)\n",
        "    fig, axes = plt.subplots(1, n_T_to_plot, figsize=(5 * n_T_to_plot, 5), squeeze=False)\n",
        "\n",
        "    for col, T_idx in enumerate(range(n_T_to_plot)):\n",
        "        ax = axes[0, col]\n",
        "        T_val = maturities[T_idx].item()\n",
        "\n",
        "        # Plot MC vs NN with the specific k-grid for this T\n",
        "        ax.plot(k_mat[T_idx, :].cpu(), iv_mc[T_idx, :].cpu(), 'b-o',\n",
        "                linewidth=2, label='True (MC)', markersize=5)\n",
        "        ax.plot(k_mat[T_idx, :].cpu(), iv_nn[T_idx, :].cpu(), 'r--s',\n",
        "                linewidth=2, label='Predicted (NN)', markersize=4)\n",
        "\n",
        "        # Formatting\n",
        "        ax.set_title(f'T = {T_val:.3f} years', fontsize=12)\n",
        "        if col == 0:\n",
        "            ax.set_ylabel('Implied Volatility', fontsize=10)\n",
        "        ax.set_xlabel('Log-Moneyness (k)', fontsize=10)\n",
        "        ax.grid(True, alpha=0.4)\n",
        "        ax.legend(fontsize=9)\n",
        "\n",
        "        # Calculate MAE for this smile\n",
        "        mae = (iv_mc[T_idx, :] - iv_nn[T_idx, :]).abs().mean().item()\n",
        "        ax.text(0.03, 0.97, f'MAE: {mae:.4f}',\n",
        "                transform=ax.transAxes,\n",
        "                verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7),\n",
        "                fontsize=8)\n",
        "\n",
        "    # Add parameters as title\n",
        "    param_names = builder.process.param_info.names if hasattr(builder.process, 'param_info') else []\n",
        "    if param_names:\n",
        "        param_str = \", \".join([f\"{name}={val:.3f}\" for name, val in zip(param_names, theta.cpu().numpy())])\n",
        "        fig.suptitle(f'MC vs NN Smile Comparison\\n{param_str}', fontsize=14)\n",
        "    else:\n",
        "        fig.suptitle('MC vs NN Smile Comparison', fontsize=14)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate and print global statistics\n",
        "    mae_global = (iv_mc - iv_nn).abs().mean().item()\n",
        "    rmse_global = ((iv_mc - iv_nn) ** 2).mean().sqrt().item()\n",
        "    max_error = (iv_mc - iv_nn).abs().max().item()\n",
        "\n",
        "    print(f\"\\nGlobal Comparison Statistics:\")\n",
        "    print(f\"  MAE:  {mae_global:.6f}\")\n",
        "    print(f\"  RMSE: {rmse_global:.6f}\")\n",
        "    print(f\"  Max Error: {max_error:.6f}\")\n",
        "\n",
        "    return iv_mc, iv_nn\n",
        "\n",
        "def visualize_surface_heatmap_comparison(iv_mc, iv_nn, maturities, logK, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualizes surface comparison as heatmap (True, Predicted, Error)\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # True surface (MC)\n",
        "    im0 = axes[0].imshow(iv_mc.cpu(), aspect='auto', cmap='viridis',\n",
        "                         extent=[logK.min().item(), logK.max().item(),\n",
        "                                maturities.min().item(), maturities.max().item()],\n",
        "                         origin='lower')\n",
        "    axes[0].set_title('True IV Surface (MC)')\n",
        "    axes[0].set_xlabel('Log-Moneyness')\n",
        "    axes[0].set_ylabel('Maturity')\n",
        "    fig.colorbar(im0, ax=axes[0])\n",
        "\n",
        "    # Predicted surface (NN)\n",
        "    im1 = axes[1].imshow(iv_nn.cpu(), aspect='auto', cmap='viridis',\n",
        "                         extent=[logK.min().item(), logK.max().item(),\n",
        "                                maturities.min().item(), maturities.max().item()],\n",
        "                         origin='lower')\n",
        "    axes[1].set_title('Predicted IV Surface (NN)')\n",
        "    axes[1].set_xlabel('Log-Moneyness')\n",
        "    fig.colorbar(im1, ax=axes[1])\n",
        "\n",
        "    # Error surface\n",
        "    error = (iv_nn - iv_mc).abs()\n",
        "    im2 = axes[2].imshow(error.cpu(), aspect='auto', cmap='hot',\n",
        "                         extent=[logK.min().item(), logK.max().item(),\n",
        "                                maturities.min().item(), maturities.max().item()],\n",
        "                         origin='lower')\n",
        "    axes[2].set_title(f'Absolute Error (max: {error.max():.4f})')\n",
        "    axes[2].set_xlabel('Log-Moneyness')\n",
        "    fig.colorbar(im2, ax=axes[2])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vrKdId_O66v7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# === CELL 5: DATASET GENERATION WITH RANDOM GRIDS ===\n",
        "# ===============================================================\n",
        "\n",
        "def generate_datasets():\n",
        "    \"\"\"Generates training and validation datasets using Random Grids/Smiles\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DATASET GENERATION WITH RANDOM GRIDS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Set seed for reproducibility\n",
        "    np.random.seed(CONFIG[\"seed\"])\n",
        "    torch.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "    # Create stochastic process\n",
        "    process = ProcessFactory.create(CONFIG[\"process\"])\n",
        "    print(f\"\\n✔ Created process: {process.__class__.__name__}\")\n",
        "    print(f\"  Parameters: {process.param_info.names if hasattr(process, 'param_info') else 'N/A'}\")\n",
        "    print(f\"  Supports absorption: {process.supports_absorption}\")\n",
        "\n",
        "    # Initialize DatasetBuilder\n",
        "    train_builder = DatasetBuilder(\n",
        "        process=process,\n",
        "        device=CONFIG[\"device\"],\n",
        "        output_dir=CONFIG[\"output_dir\"],\n",
        "        dataset_type='train'\n",
        "    )\n",
        "\n",
        "    # --- TRAINING DATASET with Random Grids ---\n",
        "    print(f\"\\n>>> Generating Training Dataset with Random Grids\")\n",
        "    print(f\"    Surfaces: {CONFIG['train_surfaces']}\")\n",
        "    print(f\"    Maturities per surface: {CONFIG['train_maturities']}\")\n",
        "    print(f\"    Strikes per maturity: {CONFIG['train_strikes']}\")\n",
        "    print(f\"    Total points: {CONFIG['train_surfaces'] * CONFIG['train_maturities'] * CONFIG['train_strikes']:,}\")\n",
        "\n",
        "    t_start = time.time()\n",
        "\n",
        "    theta_train, T_train, k_train, iv_train = train_builder.build_random_grids_dataset(\n",
        "        n_surfaces=CONFIG[\"train_surfaces\"],\n",
        "        n_maturities=CONFIG[\"train_maturities\"],\n",
        "        n_strikes=CONFIG[\"train_strikes\"],\n",
        "        n_paths=CONFIG[\"mc_paths\"],\n",
        "        spot=CONFIG[\"spot\"],\n",
        "        normalize=True,\n",
        "        compute_stats_from=None,\n",
        "        show_progress=True,\n",
        "        batch_size=50,\n",
        "        checkpoint_every=CONFIG.get(\"dataset_checkpoint_every\", 100),\n",
        "        resume_from=('latest' if CONFIG.get(\"resume_dataset_generation\", True) else None),\n",
        "        save_all_thetas=True,\n",
        "        base_seed=CONFIG.get(\"seed\", 42)\n",
        "    )\n",
        "\n",
        "    train_time = time.time() - t_start\n",
        "    print(f\"✔ Training dataset generated in {train_time:.1f}s\")\n",
        "    print_dataset_stats(theta_train, T_train, k_train, iv_train, \"Training Dataset\")\n",
        "\n",
        "    # Initialize DatasetBuilder\n",
        "    val_builder = DatasetBuilder(\n",
        "        process=process,\n",
        "        device=CONFIG[\"device\"],\n",
        "        output_dir=CONFIG[\"output_dir\"],\n",
        "        dataset_type='val'\n",
        "    )\n",
        "    # --- VALIDATION DATASET with Random Smiles ---\n",
        "    print(f\"\\n>>> Generating Validation Dataset with Random Smiles\")\n",
        "    print(f\"    Smiles: {CONFIG['val_smiles']}\")\n",
        "    print(f\"    Strikes per smile: {CONFIG['val_strikes']}\")\n",
        "    print(f\"    Total points: {CONFIG['val_smiles'] * CONFIG['val_strikes']:,}\")\n",
        "\n",
        "    t_start = time.time()\n",
        "\n",
        "    theta_val, T_val, k_val, iv_val = val_builder.build_random_smiles_dataset(\n",
        "        n_smiles=CONFIG[\"val_smiles\"],\n",
        "        n_strikes_per_smile=CONFIG[\"val_strikes\"],\n",
        "        n_paths=CONFIG[\"mc_paths\"],\n",
        "        spot=CONFIG[\"spot\"],\n",
        "        normalize=True,\n",
        "        compute_stats_from=train_builder,  # Use training statistics\n",
        "        show_progress=True,\n",
        "        checkpoint_every=CONFIG.get(\"dataset_checkpoint_every\", 100),\n",
        "        resume_from=('latest' if CONFIG.get(\"resume_dataset_generation\", True) else None),\n",
        "        base_seed=CONFIG.get(\"seed\", 42)  # for consistency with training\n",
        "    )\n",
        "\n",
        "    val_time = time.time() - t_start\n",
        "    print(f\"✔ Validation dataset generated in {val_time:.1f}s\")\n",
        "    print_dataset_stats(theta_val, T_val, k_val, iv_val, \"Validation Dataset\")\n",
        "\n",
        "    # Save datasets\n",
        "    print(f\"\\n>>> Saving datasets to {CONFIG['output_dir']}/datasets/\")\n",
        "\n",
        "    torch.save({\n",
        "        'theta': theta_train, 'T': T_train, 'k': k_train, 'iv': iv_train,\n",
        "        'theta_mean': train_builder.theta_mean, 'theta_std': train_builder.theta_std,\n",
        "        'T_mean': train_builder.T_mean, 'T_std': train_builder.T_std,\n",
        "        'k_mean': train_builder.k_mean, 'k_std': train_builder.k_std,\n",
        "        'iv_mean': train_builder.iv_mean, 'iv_std': train_builder.iv_std,\n",
        "        'config': CONFIG\n",
        "    }, f\"{CONFIG['output_dir']}/datasets/train_random_grids.pt\")\n",
        "\n",
        "    torch.save({\n",
        "        'theta': theta_val, 'T': T_val, 'k': k_val, 'iv': iv_val\n",
        "    }, f\"{CONFIG['output_dir']}/datasets/val_random_smiles.pt\")\n",
        "\n",
        "    print(\"✔ Datasets saved\")\n",
        "\n",
        "    # Visualize some examples\n",
        "    visualize_random_grids_sample(train_builder, theta_train, T_train, k_train, iv_train, n_samples=2)\n",
        "\n",
        "    # Cleanup dataset checkpoints (optional via CONFIG)\n",
        "    if CONFIG.get(\"cleanup_checkpoints_after_completion\", True):\n",
        "        cleanup_dataset_checkpoints(f\"{CONFIG['output_dir']}/checkpoints/random_grids\")\n",
        "\n",
        "    return train_builder, (theta_train, T_train, k_train, iv_train), (theta_val, T_val, k_val, iv_val)"
      ],
      "metadata": {
        "id": "qmK0VFbb7IhR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# === CELL 6: POINTWISE NETWORK TRAINING ===\n",
        "# ===============================================================\n",
        "\n",
        "def train_pointwise_network(builder, train_data, val_data):\n",
        "    \"\"\"PointwiseNetworkPricer training\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"POINTWISE NETWORK TRAINING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    theta_train, T_train, k_train, iv_train = train_data\n",
        "    theta_val, T_val, k_val, iv_val = val_data\n",
        "\n",
        "    # Create the pricer\n",
        "    pricer = PointwiseNetworkPricer(\n",
        "        process=builder.process,\n",
        "        hidden_layers=CONFIG[\"hidden_layers\"],\n",
        "        activation=CONFIG[\"activation\"],\n",
        "        device=CONFIG[\"device\"],\n",
        "        enable_smile_repair=False  # Not necessary for pointwise\n",
        "    )\n",
        "\n",
        "    print(f\"\\n✔ Created PointwiseNetworkPricer\")\n",
        "    print(f\"  Architecture: {builder.process.num_params} + 2 → {CONFIG['hidden_layers']} → 1\")\n",
        "    print(f\"  Activation: {CONFIG['activation']}\")\n",
        "    print(f\"  Total parameters: {sum(p.numel() for p in pricer.net.parameters()):,}\")\n",
        "\n",
        "    # Set normalization statistics\n",
        "    pricer.set_normalization_stats(\n",
        "        builder.theta_mean,\n",
        "        builder.theta_std,\n",
        "        builder.iv_mean,\n",
        "        builder.iv_std\n",
        "    )\n",
        "    pricer.set_pointwise_normalization_stats(\n",
        "        builder.T_mean,\n",
        "        builder.T_std,\n",
        "        builder.k_mean,\n",
        "        builder.k_std\n",
        "    )\n",
        "\n",
        "    # Setup training\n",
        "    use_cuda = str(CONFIG[\"device\"]).startswith(\"cuda\")\n",
        "    num_workers = 2 if use_cuda else 0\n",
        "    train_loader = DataLoader(\n",
        "        TensorDataset(theta_train, T_train, k_train, iv_train),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=0,  # ALWAYS 0 to avoid CUDA issues\n",
        "        pin_memory=False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        TensorDataset(theta_val, T_val, k_val, iv_val),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=0,  # ALWAYS 0\n",
        "        pin_memory=False\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.Adam(pricer.net.parameters(), lr=CONFIG[\"learning_rate\"])\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    if CONFIG[\"lr_scheduler\"]:\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=10\n",
        "        )\n",
        "\n",
        "    # ---- Resume / Warm-start --------------------------------------------\n",
        "    def _extract_model_state(ckpt_obj):\n",
        "        # 1) raw OrderedDict (state_dict saved directly)\n",
        "        from collections.abc import Mapping\n",
        "        if isinstance(ckpt_obj, Mapping):\n",
        "            # prefer known keys\n",
        "            for key in (\"model_state_dict\", \"state_dict\", \"net_state_dict\", \"pricer_state_dict\"):\n",
        "                sd = ckpt_obj.get(key, None)\n",
        "                if isinstance(sd, Mapping):\n",
        "                    return sd\n",
        "            # if all fail, perhaps the entire ckpt is already a state_dict\n",
        "            if all(hasattr(v, \"shape\") for v in ckpt_obj.values()):\n",
        "                return ckpt_obj  # raw state_dict\n",
        "        return None\n",
        "\n",
        "    def _strip_module_prefix(sd):\n",
        "        if any(k.startswith(\"module.\") for k in sd.keys()):\n",
        "            return {k.replace(\"module.\", \"\", 1): v for k, v in sd.items()}\n",
        "        return sd\n",
        "    start_epoch = 1\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_weights = None\n",
        "    best_epoch = 0\n",
        "\n",
        "    if CONFIG.get(\"resume_training\", False):\n",
        "        import glob, re, os\n",
        "        ckpt_path = CONFIG.get(\"resume_from\")\n",
        "        if ckpt_path in (None, \"latest\"):\n",
        "            ckpt_dir = f\"{CONFIG['output_dir']}/models/checkpoints\"\n",
        "            ckpts = glob.glob(os.path.join(ckpt_dir, \"checkpoint_epoch_*.pt\"))\n",
        "            if ckpts:\n",
        "                ckpt_path = max(\n",
        "                    ckpts,\n",
        "                    key=lambda p: int(re.search(r\"epoch_(\\d+)\", p).group(1))\n",
        "                )\n",
        "        if ckpt_path and os.path.exists(ckpt_path):\n",
        "            ckpt = torch.load(ckpt_path, map_location=CONFIG[\"device\"])\n",
        "            model_sd = _extract_model_state(ckpt)\n",
        "            if model_sd is None:\n",
        "                raise ValueError(\n",
        "                    f\"Checkpoint '{ckpt_path}' does not contain a model state_dict. \"\n",
        "                    f\"Available keys: {list(ckpt.keys()) if isinstance(ckpt, dict) else type(ckpt)}\"\n",
        "                )\n",
        "            model_sd = _strip_module_prefix(model_sd)\n",
        "            pricer.net.load_state_dict(model_sd, strict=False)\n",
        "            opt_sd = ckpt.get(\"optimizer_state_dict\", None) if isinstance(ckpt, dict) else None\n",
        "            if opt_sd:\n",
        "                optimizer.load_state_dict(opt_sd)\n",
        "            else:\n",
        "                print(\">>> Checkpoint has no optimizer state; continuing with fresh optimizer.\")\n",
        "            start_epoch = int(ckpt[\"epoch\"]) + 1\n",
        "            # retrieve history (if present) for reasonable best_*\n",
        "            train_losses = ckpt.get(\"train_losses\", [])\n",
        "            val_losses = ckpt.get(\"val_losses\", [])\n",
        "            if val_losses:\n",
        "                best_val_loss = min(val_losses)\n",
        "                best_epoch = val_losses.index(best_val_loss) + 1\n",
        "                best_weights = pricer.net.state_dict().copy()\n",
        "            print(f\">>> Resuming from {ckpt_path} (start_epoch={start_epoch})\")\n",
        "    elif CONFIG.get(\"warm_start\", False):\n",
        "        import os\n",
        "        wp = CONFIG.get(\"warm_start_path\")\n",
        "        if not wp:\n",
        "            # prefer the last best saved by the script\n",
        "            cand = f\"{CONFIG['output_dir']}/models/best_model.pt\"\n",
        "            wp = cand if os.path.exists(cand) else f\"{CONFIG['output_dir']}/models/pointwise_random_grids_latest.pt\"\n",
        "        if os.path.exists(wp):\n",
        "            sd = torch.load(wp, map_location=CONFIG[\"device\"])\n",
        "            state = sd.get(\"model_state_dict\", sd)\n",
        "            pricer.net.load_state_dict(state, strict=False)\n",
        "            best_weights = pricer.net.state_dict().copy()  # starting point\n",
        "            print(f\">>> Warm-start from {wp}\")\n",
        "\n",
        "    # If the checkpoint is already beyond the requested number of epochs, exit cleanly\n",
        "    if start_epoch > CONFIG[\"epochs\"]:\n",
        "        # try to derive best\n",
        "        if val_losses:\n",
        "            best_val_loss = float(min(val_losses))\n",
        "            best_epoch    = 1 + val_losses.index(min(val_losses))\n",
        "        else:\n",
        "            # fallback: if ckpt had 'val_loss' and 'epoch'\n",
        "            best_val_loss = float(ckpt.get('val_loss', float('nan'))) if isinstance(ckpt, dict) else float('nan')\n",
        "            best_epoch    = int(ckpt.get('epoch', start_epoch-1))     if isinstance(ckpt, dict) else start_epoch-1\n",
        "        history = {\n",
        "            \"train_losses\": train_losses,\n",
        "            \"val_losses\": val_losses,\n",
        "            \"best_val_loss\": best_val_loss,\n",
        "            \"best_epoch\": best_epoch,\n",
        "            \"start_epoch\": start_epoch,\n",
        "            \"epochs\": CONFIG[\"epochs\"],\n",
        "            \"resumed\": True,\n",
        "        }\n",
        "        return pricer, history\n",
        "\n",
        "\n",
        "    # Training loop with early stopping\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"\\n>>> Starting training for {CONFIG['epochs']} epochs\")\n",
        "    print(f\"    Batch size: {CONFIG['batch_size']}\")\n",
        "    print(f\"    Learning rate: {CONFIG['learning_rate']}\")\n",
        "\n",
        "    for epoch in range(start_epoch, CONFIG[\"epochs\"] + 1):\n",
        "        # Training\n",
        "        pricer.net.train()\n",
        "        train_loss = 0.0\n",
        "        n_train_batches = 0\n",
        "\n",
        "        for theta_batch, T_batch, k_batch, iv_batch in train_loader:\n",
        "            # Forward pass\n",
        "            pred = pricer.forward(theta_batch, T_batch, k_batch)\n",
        "            loss = loss_fn(pred, iv_batch)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(pricer.net.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * len(theta_batch)\n",
        "            n_train_batches += len(theta_batch)\n",
        "\n",
        "        train_loss /= n_train_batches\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        pricer.net.eval()\n",
        "        val_loss = 0.0\n",
        "        n_val_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for theta_batch, T_batch, k_batch, iv_batch in val_loader:\n",
        "                pred = pricer.forward(theta_batch, T_batch, k_batch)\n",
        "                loss = loss_fn(pred, iv_batch)\n",
        "                val_loss += loss.item() * len(theta_batch)\n",
        "                n_val_batches += len(theta_batch)\n",
        "\n",
        "        val_loss /= n_val_batches\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        if CONFIG[\"lr_scheduler\"]:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "        # Check for best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_weights = pricer.net.state_dict().copy()\n",
        "            best_epoch = epoch\n",
        "            patience_counter = 0\n",
        "\n",
        "            # Save best model\n",
        "            torch.save({\n",
        "                'model_state_dict': best_weights,\n",
        "                'epoch': best_epoch,\n",
        "                'val_loss': best_val_loss,\n",
        "                'config': CONFIG,\n",
        "                'normalization_stats': {\n",
        "                    'theta_mean': builder.theta_mean,\n",
        "                    'theta_std': builder.theta_std,\n",
        "                    'T_mean': builder.T_mean,\n",
        "                    'T_std': builder.T_std,\n",
        "                    'k_mean': builder.k_mean,\n",
        "                    'k_std': builder.k_std,\n",
        "                    'iv_mean': builder.iv_mean,\n",
        "                    'iv_std': builder.iv_std\n",
        "                }\n",
        "            }, f\"{CONFIG['output_dir']}/models/best_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Logging\n",
        "        if epoch % 5 == 0 or epoch == 1:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"Epoch {epoch:3d}/{CONFIG['epochs']} | \"\n",
        "                  f\"Train: {train_loss:.6f} | Val: {val_loss:.6f} | \"\n",
        "                  f\"Best: {best_val_loss:.6f} (ep {best_epoch}) | \"\n",
        "                  f\"LR: {current_lr:.1e}\")\n",
        "\n",
        "        # Periodic checkpoint\n",
        "        # Default/fallback if missing in CONFIG\n",
        "        checkpoint_every = CONFIG.get(\"checkpoint_every\", 100)\n",
        "        if checkpoint_every and (epoch % checkpoint_every == 0):\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': pricer.net.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses\n",
        "            }\n",
        "            torch.save(checkpoint,\n",
        "                      f\"{CONFIG['output_dir']}/models/checkpoints/checkpoint_epoch_{epoch}.pt\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= CONFIG[\"early_stopping_patience\"]:\n",
        "            print(f\"\\n>>> Early stopping triggered at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "        # Clear cache periodically\n",
        "        if epoch % 10 == 0:\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    # Load best weights\n",
        "    pricer.net.load_state_dict(best_weights)\n",
        "    print(f\"\\n✔ Training completed. Best model from epoch {best_epoch} (Val Loss: {best_val_loss:.6f})\")\n",
        "\n",
        "    # Save training history\n",
        "    history = {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'best_epoch': best_epoch,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'config': CONFIG\n",
        "    }\n",
        "\n",
        "    with open(f\"{CONFIG['output_dir']}/logs/training_history.pkl\", 'wb') as f:\n",
        "        pickle.dump(history, f)\n",
        "\n",
        "    # Plot training curves\n",
        "    plot_training_history(train_losses, val_losses,\n",
        "                         f\"{CONFIG['output_dir']}/visualizations/training_curves.png\")\n",
        "\n",
        "    return pricer, history"
      ],
      "metadata": {
        "id": "2-I2ooW37Rx_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# === CELL 7: TESTING AND INFERENCE ===\n",
        "# ===============================================================\n",
        "\n",
        "def _training_k_bounds_for_T(builder, T, spot: float = 1.0):\n",
        "    \"\"\"\n",
        "    Returns (k_min, k_max) of the training domain for a given T.\n",
        "    Uses the same parameters as the DatasetBuilder: K_min = S(1 - l√T), K_max = S(1 + u√T)\n",
        "    with numerical clamps [0.05S, 3S], then k = log(K/S).\n",
        "    \"\"\"\n",
        "    sp = getattr(builder, \"_strike_params\", None) or {\"l\": 0.55, \"u\": 0.30}\n",
        "    l, u = float(sp[\"l\"]), float(sp[\"u\"])\n",
        "    T_t = torch.as_tensor(T, dtype=torch.float32, device=builder.device)\n",
        "    sqrtT = torch.sqrt(T_t)\n",
        "    K_min = torch.clamp(torch.tensor(spot, device=builder.device) * (1.0 - l * sqrtT), min=0.05 * spot)\n",
        "    K_max = torch.clamp(torch.tensor(spot, device=builder.device) * (1.0 + u * sqrtT), max=3.0 * spot)\n",
        "    k_min = torch.log(K_min / spot).item()\n",
        "    k_max = torch.log(K_max / spot).item()\n",
        "    return k_min, k_max\n",
        "\n",
        "def make_in_domain_logk_grid(builder, maturities: torch.Tensor, n_points: int = 11,\n",
        "                             safety_eps: float = 1e-3, spot: float = 1.0) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Grid of k that is within the INTERSECTION of the training domains for all passed Ts.\n",
        "    Avoids extrapolation in tests/heatmaps.\n",
        "    \"\"\"\n",
        "    kmins, kmaxs = [], []\n",
        "    for T in maturities:\n",
        "        lo, hi = _training_k_bounds_for_T(builder, T, spot=spot)\n",
        "        kmins.append(lo); kmaxs.append(hi)\n",
        "    k_lo = max(kmins) + safety_eps\n",
        "    k_hi = min(kmaxs) - safety_eps\n",
        "    if not (k_lo < k_hi):\n",
        "        raise ValueError(\"No overlap of training domains between the provided maturities.\")\n",
        "    return torch.linspace(k_lo, k_hi, n_points, device=builder.device, dtype=torch.float32)\n",
        "\n",
        "def test_and_inference(pricer, builder, val_data):\n",
        "    \"\"\"Model testing and inference examples\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MODEL TESTING AND INFERENCE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    theta_val, T_val, k_val, iv_val = val_data\n",
        "\n",
        "    # Test on a subset of the validation set\n",
        "    n_test = min(1000, len(theta_val))\n",
        "    test_idx = torch.randperm(len(theta_val))[:n_test]\n",
        "\n",
        "    theta_test = theta_val[test_idx]\n",
        "    T_test = T_val[test_idx]\n",
        "    k_test = k_val[test_idx]\n",
        "    iv_test = iv_val[test_idx]\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae, rmse, mape = test_predictions(pricer, builder,\n",
        "                                       theta_test, T_test, k_test, iv_test,\n",
        "                                       n_samples=5)\n",
        "\n",
        "    # --- MC vs NN Comparison ---\n",
        "    print(\"\\n>>> Generating MC vs NN Comparison\")\n",
        "\n",
        "    # Select random parameters for comparison\n",
        "    test_theta_mc = builder.sample_theta_lhs(1)[0]\n",
        "\n",
        "    # Define grid for comparison\n",
        "    test_maturities = torch.tensor([0.08, 0.17, 0.25, 0.5, 1.0], device=CONFIG[\"device\"])\n",
        "    # test_logK = torch.linspace(-0.3, 0.3, 11, device=CONFIG[\"device\"]) # Fixed grid\n",
        "    test_logK = make_in_domain_logk_grid(builder, test_maturities, n_points=11)\n",
        "\n",
        "\n",
        "    # Generate comparison\n",
        "    iv_mc, iv_nn = visualize_mc_vs_nn_comparison(\n",
        "        builder, pricer, test_theta_mc,\n",
        "        test_maturities, test_logK,\n",
        "        n_mc_paths=50000,\n",
        "        save_path=f\"{CONFIG['output_dir']}/visualizations/mc_vs_nn_comparison.png\"\n",
        "    )\n",
        "\n",
        "    # --- Inference test on new points ---\n",
        "    print(\"\\n>>> Testing inference on new random points\")\n",
        "\n",
        "    # Generate some random parameters\n",
        "    test_params = builder.sample_theta_lhs(3)\n",
        "    print(f\"\\nTest parameters (denormalized):\")\n",
        "    for i, params in enumerate(test_params):\n",
        "        param_str = \", \".join([f\"{p:.3f}\" for p in params.cpu().numpy()])\n",
        "        print(f\"  Sample {i+1}: [{param_str}]\")\n",
        "\n",
        "    # Generate random points (T, k)\n",
        "    test_T = torch.tensor([0.1, 0.25, 0.5, 1.0, 2.0], device=CONFIG[\"device\"])\n",
        "    test_k = torch.tensor([-0.2, -0.1, 0.0, 0.1, 0.2], device=CONFIG[\"device\"])\n",
        "\n",
        "    # Create full grid\n",
        "    test_theta_grid = test_params[0].unsqueeze(0).repeat(len(test_T) * len(test_k), 1)\n",
        "    T_grid, k_grid = torch.meshgrid(test_T, test_k, indexing='ij')\n",
        "    T_flat = T_grid.flatten()\n",
        "    k_flat = k_grid.flatten()\n",
        "\n",
        "    # Prediction\n",
        "    with torch.no_grad():\n",
        "        iv_pred = pricer.price_iv(test_theta_grid, T_flat, k_flat, denormalize_output=True)\n",
        "\n",
        "    # Visualize predicted surface\n",
        "    iv_surface = iv_pred.reshape(len(test_T), len(test_k))\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # 3D surface plot\n",
        "    ax1 = fig.add_subplot(121, projection='3d')\n",
        "    T_mesh, k_mesh = np.meshgrid(test_T.cpu(), test_k.cpu(), indexing='ij')\n",
        "    surf = ax1.plot_surface(k_mesh, T_mesh, iv_surface.cpu().numpy(),\n",
        "                           cmap='viridis', alpha=0.9)\n",
        "    ax1.set_xlabel('Log-Moneyness')\n",
        "    ax1.set_ylabel('Maturity')\n",
        "    ax1.set_zlabel('Implied Volatility')\n",
        "    ax1.set_title('Predicted IV Surface')\n",
        "    fig.colorbar(surf, ax=ax1, shrink=0.5)\n",
        "\n",
        "    # Heatmap\n",
        "    ax2 = fig.add_subplot(122)\n",
        "    im = ax2.imshow(iv_surface.cpu().numpy(), aspect='auto', cmap='viridis',\n",
        "                    extent=[test_k.min().item(), test_k.max().item(),\n",
        "                           test_T.min().item(), test_T.max().item()])\n",
        "    ax2.set_xlabel('Log-Moneyness')\n",
        "    ax2.set_ylabel('Maturity')\n",
        "    ax2.set_title('IV Surface Heatmap')\n",
        "    fig.colorbar(im, ax=ax2)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{CONFIG['output_dir']}/visualizations/inference_surface.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n✔ Inference test completed\")\n",
        "    print(f\"  Surface shape: {iv_surface.shape}\")\n",
        "    print(f\"  IV range: [{iv_surface.min():.4f}, {iv_surface.max():.4f}]\")\n",
        "    print(f\"  Mean IV: {iv_surface.mean():.4f} ± {iv_surface.std():.4f}\")\n",
        "\n",
        "    return mae, rmse, mape"
      ],
      "metadata": {
        "id": "InWfLCA87VX3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# === CELL 8: SAVE & LOAD UTILITIES ===\n",
        "# ===============================================================\n",
        "\n",
        "def save_final_model(pricer, builder, history):\n",
        "    \"\"\"Saves the final model with all metadata\"\"\"\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    process_name = builder.process.__class__.__name__.lower()\n",
        "\n",
        "    final_path = f\"{CONFIG['output_dir']}/models/final\"\n",
        "    model_name = f\"{process_name}_pointwise_random_grids_{timestamp}\"\n",
        "\n",
        "    # Save the complete model\n",
        "    model_dict = {\n",
        "        'model_state_dict': pricer.net.state_dict(),\n",
        "        'process_name': builder.process.__class__.__name__,\n",
        "        'process_key': CONFIG['process'],\n",
        "        'process_params': {\n",
        "            'num_params': builder.process.num_params,\n",
        "            'param_names': builder.process.param_info.names if hasattr(builder.process, 'param_info') else [],\n",
        "            'param_bounds': builder.process.param_info.bounds if hasattr(builder.process, 'param_info') else [],\n",
        "        },\n",
        "        'network_config': {\n",
        "            'hidden_layers': CONFIG['hidden_layers'],\n",
        "            'activation': CONFIG['activation'],\n",
        "        },\n",
        "        'normalization_stats': {\n",
        "            'theta_mean': builder.theta_mean.cpu(),\n",
        "            'theta_std': builder.theta_std.cpu(),\n",
        "            'T_mean': builder.T_mean.cpu(),\n",
        "            'T_std': builder.T_std.cpu(),\n",
        "            'k_mean': builder.k_mean.cpu(),\n",
        "            'k_std': builder.k_std.cpu(),\n",
        "            'iv_mean': builder.iv_mean.cpu(),\n",
        "            'iv_std': builder.iv_std.cpu(),\n",
        "        },\n",
        "        'training_config': CONFIG,\n",
        "        'training_history': history,\n",
        "        'timestamp': timestamp\n",
        "    }\n",
        "\n",
        "    # Save as .pt\n",
        "    torch.save(model_dict, f\"{final_path}/{model_name}.pt\")\n",
        "\n",
        "    # extract fields robustly\n",
        "    train_losses = history.get('train_losses', [])\n",
        "    val_losses   = history.get('val_losses', [])\n",
        "    if 'best_val_loss' in history and 'best_epoch' in history:\n",
        "        best_val_loss = float(history['best_val_loss'])\n",
        "        best_epoch    = int(history['best_epoch'])\n",
        "    elif val_losses:\n",
        "        best_val_loss = float(min(val_losses))\n",
        "        best_epoch    = 1 + val_losses.index(min(val_losses))\n",
        "    else:\n",
        "        best_val_loss = float('nan')\n",
        "        best_epoch    = -1\n",
        "\n",
        "\n",
        "    # Also save configuration as JSON for reference\n",
        "    config_dict = {\n",
        "        'model_name': model_name,\n",
        "        'process_name': builder.process.__class__.__name__,\n",
        "        'process_key': CONFIG['process'],\n",
        "        'network_architecture': f\"{builder.process.num_params}+2 → {CONFIG['hidden_layers']} → 1\",\n",
        "        'training_config': CONFIG,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'best_epoch': best_epoch,\n",
        "        'total_epochs': len(train_losses),\n",
        "        'timestamp': timestamp\n",
        "    }\n",
        "\n",
        "\n",
        "    with open(f\"{final_path}/{model_name}_config.json\", 'w') as f:\n",
        "        json.dump(config_dict, f, indent=2)\n",
        "\n",
        "    # Copy as \"latest\" for easy access\n",
        "    import shutil\n",
        "    shutil.copy2(f\"{final_path}/{model_name}.pt\",\n",
        "                 f\"{final_path}/pointwise_random_grids_latest.pt\")\n",
        "    shutil.copy2(f\"{final_path}/{model_name}_config.json\",\n",
        "                 f\"{final_path}/pointwise_random_grids_latest_config.json\")\n",
        "\n",
        "    print(f\"\\n✔ Model saved to Google Drive:\")\n",
        "    print(f\"  Directory: {final_path}\")\n",
        "    print(f\"  Model: {model_name}.pt\")\n",
        "    print(f\"  Config: {model_name}_config.json\")\n",
        "\n",
        "    return model_name\n",
        "\n",
        "\n",
        "def load_saved_model(model_path, device='cpu'):\n",
        "    \"\"\"Loads a saved model\"\"\"\n",
        "\n",
        "    print(f\"\\nLoading model from: {model_path}\")\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "    # Recreate the process from the saved key (fallback if absent)\n",
        "    process_key = checkpoint.get('process_key')\n",
        "    if process_key is None:\n",
        "        name = str(checkpoint.get('process_name', '')).lower()\n",
        "        if 'rough' in name and 'bergomi' in name:\n",
        "            process_key = 'rough_bergomi'\n",
        "        elif 'rough' in name and 'heston' in name:\n",
        "            process_key = 'rough_heston'\n",
        "        elif 'heston' in name:\n",
        "            process_key = 'heston'\n",
        "        else:\n",
        "            raise ValueError(f\"Unrecognized process key from '{name}'\")\n",
        "    process = ProcessFactory.create(process_key)\n",
        "\n",
        "    # Recreate the pricer\n",
        "    loaded_pricer = PointwiseNetworkPricer(\n",
        "        process=process,\n",
        "        hidden_layers=checkpoint['network_config']['hidden_layers'],\n",
        "        activation=checkpoint['network_config']['activation'],\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Load weights\n",
        "    loaded_pricer.net.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Set normalization statistics\n",
        "    norm_stats = checkpoint['normalization_stats']\n",
        "    loaded_pricer.set_normalization_stats(\n",
        "        norm_stats['theta_mean'].to(device),\n",
        "        norm_stats['theta_std'].to(device),\n",
        "        norm_stats['iv_mean'].to(device),\n",
        "        norm_stats['iv_std'].to(device)\n",
        "    )\n",
        "    loaded_pricer.set_pointwise_normalization_stats(\n",
        "        norm_stats['T_mean'].to(device),\n",
        "        norm_stats['T_std'].to(device),\n",
        "        norm_stats['k_mean'].to(device),\n",
        "        norm_stats['k_std'].to(device)\n",
        "    )\n",
        "\n",
        "\n",
        "    loaded_pricer.eval()\n",
        "\n",
        "    print(f\"✔ Model loaded successfully\")\n",
        "    print(f\"  Process: {process_key} ({checkpoint.get('process_name')})\")\n",
        "    print(f\"  Architecture: {checkpoint['network_config']['hidden_layers']}\")\n",
        "    print(f\"  Best val loss: {checkpoint['training_history']['best_val_loss']:.6f}\")\n",
        "\n",
        "\n",
        "    return loaded_pricer, checkpoint"
      ],
      "metadata": {
        "id": "yZXGnnfS7ZzS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# === CELL 9: MAIN FUNCTION ===\n",
        "# ===============================================================\n",
        "\n",
        "def run_complete_random_grids_training():\n",
        "    \"\"\"Runs the complete training with Random Grids\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" RANDOM GRIDS POINTWISE NETWORK TRAINING \")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nConfiguration:\")\n",
        "    print(f\"  Process: {CONFIG['process']}\")\n",
        "    print(f\"  Device: {CONFIG['device']}\")\n",
        "    print(f\"  Training surfaces: {CONFIG['train_surfaces']}\")\n",
        "    print(f\"  Validation smiles: {CONFIG['val_smiles']}\")\n",
        "    print(f\"  Network: {CONFIG['hidden_layers']}\")\n",
        "    print(f\"  Output: {CONFIG['output_dir']}\")\n",
        "\n",
        "    # Step 1: Generate datasets\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 1: DATASET GENERATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    builder, train_data, val_data = generate_datasets()\n",
        "\n",
        "    # Cleanup memory after dataset generation\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Step 2: Training\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 2: NETWORK TRAINING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    pricer, history = train_pointwise_network(builder, train_data, val_data)\n",
        "\n",
        "    # Step 3: Testing\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 3: MODEL TESTING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    mae, rmse, mape = test_and_inference(pricer, builder, val_data)\n",
        "\n",
        "    # Step 3b: MC vs NN Detailed Comparison\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 3b: MC VS NN COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Generate detailed comparison\n",
        "    comparison_theta = builder.sample_theta_lhs(1)[0]\n",
        "    comparison_maturities = torch.tensor([0.08, 0.17, 0.25, 0.5, 1.0, 2.0], device=CONFIG[\"device\"])\n",
        "    # comparison_logK = torch.linspace(-0.3, 0.3, 13, device=CONFIG[\"device\"]) # Fixed grid\n",
        "    comparison_logK = make_in_domain_logk_grid(builder, comparison_maturities, n_points=13)\n",
        "\n",
        "    print(f\"\\nGenerating detailed comparison with:\")\n",
        "    print(f\"  Maturities: {comparison_maturities.cpu().numpy()}\")\n",
        "    print(f\"  Strikes: {len(comparison_logK)} points from {comparison_logK.min():.2f} to {comparison_logK.max():.2f}\")\n",
        "\n",
        "    iv_mc_ref, iv_nn_pred = visualize_mc_vs_nn_comparison(\n",
        "        builder, pricer, comparison_theta,\n",
        "        comparison_maturities, comparison_logK,\n",
        "        n_mc_paths=100000,  # More paths for accurate reference\n",
        "        save_path=f\"{CONFIG['output_dir']}/visualizations/smile_comparison_detailed.png\"\n",
        "    )\n",
        "\n",
        "    # Add heatmap comparison as well\n",
        "    visualize_surface_heatmap_comparison(\n",
        "        iv_mc_ref, iv_nn_pred,\n",
        "        comparison_maturities, comparison_logK,\n",
        "        save_path=f\"{CONFIG['output_dir']}/visualizations/surface_comparison_heatmap.png\"\n",
        "    )\n",
        "\n",
        "\n",
        "    # Step 4: Save model\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 4: SAVING MODEL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model_name = save_final_model(pricer, builder, history)\n",
        "\n",
        "    # Step 5: Verify loading\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 5: TESTING MODEL LOADING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model_path = f\"{CONFIG['output_dir']}/models/final/pointwise_random_grids_latest.pt\"\n",
        "    loaded_pricer, loaded_checkpoint = load_saved_model(model_path, device=CONFIG['device'])\n",
        "\n",
        "    # Test that the loaded model gives the same results\n",
        "    print(\"\\n>>> Verifying loaded model...\")\n",
        "    test_theta = torch.randn(5, builder.process.num_params, device=CONFIG['device'])\n",
        "    test_T = torch.rand(5, device=CONFIG['device']) * 2\n",
        "    test_k = torch.randn(5, device=CONFIG['device']) * 0.3\n",
        "\n",
        "\n",
        "    pricer.eval()\n",
        "    with torch.no_grad():\n",
        "        original_pred = pricer.price_iv(test_theta, test_T, test_k, denormalize_output=True)\n",
        "        loaded_pred = loaded_pricer.price_iv(test_theta, test_T, test_k, denormalize_output=True)\n",
        "\n",
        "        diff = (original_pred - loaded_pred).abs().max()\n",
        "        print(f\"  Max difference: {diff:.8f}\")\n",
        "\n",
        "        if diff < 1e-6:\n",
        "            print(\"  ✔ Loaded model produces identical results\")\n",
        "        else:\n",
        "            print(\"  ⚠ Warning: Loaded model produces different results\")\n",
        "\n",
        "    # Final Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING COMPLETED SUCCESSFULLY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"  Best Validation Loss: {history['best_val_loss']:.6f}\")\n",
        "    print(f\"  Best Epoch: {history['best_epoch']}/{len(history['train_losses'])}\")\n",
        "    print(f\"  Test MAE: {mae:.6f}\")\n",
        "    print(f\"  Test RMSE: {rmse:.6f}\")\n",
        "    print(f\"  Test MAPE: {mape:.2f}%\")\n",
        "    print(f\"\\nModel saved as: {model_name}\")\n",
        "    print(f\"Location: {CONFIG['output_dir']}/models/final/\")\n",
        "\n",
        "    return pricer, builder, history"
      ],
      "metadata": {
        "id": "W2siGRrO7a4o"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# === CELL 10: UTILITY FOR SURFACE GENERATION ===\n",
        "# ===============================================================\n",
        "\n",
        "def generate_full_surface(pricer, builder, theta, T_range=(0.01, 3.0), k_range=(-0.5, 0.5), n_points=50):\n",
        "    \"\"\"\n",
        "    Generates a complete IV surface for a given set of parameters\n",
        "\n",
        "    Args:\n",
        "        pricer: The trained model\n",
        "        builder: The DatasetBuilder for denormalization\n",
        "        theta: Model parameters (not normalized)\n",
        "        T_range: Range of maturities\n",
        "        k_range: Range of log-moneyness\n",
        "        n_points: Number of points per dimension\n",
        "    \"\"\"\n",
        "\n",
        "    # Create grid\n",
        "    T_grid = torch.linspace(T_range[0], T_range[1], n_points, device=CONFIG['device'])\n",
        "    k_grid = torch.linspace(k_range[0], k_range[1], n_points, device=CONFIG['device'])\n",
        "\n",
        "    # Meshgrid\n",
        "    T_mesh, k_mesh = torch.meshgrid(T_grid, k_grid, indexing='ij')\n",
        "    T_flat = T_mesh.flatten()\n",
        "    k_flat = k_mesh.flatten()\n",
        "\n",
        "    # Expand theta for all points\n",
        "    theta_expanded = theta.unsqueeze(0).repeat(len(T_flat), 1)\n",
        "\n",
        "    # Prediction\n",
        "    pricer.eval()\n",
        "    with torch.no_grad():\n",
        "        iv_flat = pricer.price_iv(theta_expanded, T_flat, k_flat, denormalize_output=True)\n",
        "\n",
        "    # Reshape to surface\n",
        "    iv_surface = iv_flat.reshape(n_points, n_points)\n",
        "\n",
        "    # Visualize\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # 3D plot\n",
        "    ax1 = fig.add_subplot(131, projection='3d')\n",
        "    surf = ax1.plot_surface(k_mesh.cpu(), T_mesh.cpu(), iv_surface.cpu(),\n",
        "                           cmap='viridis', alpha=0.9)\n",
        "    ax1.set_xlabel('Log-Moneyness')\n",
        "    ax1.set_ylabel('Maturity (years)')\n",
        "    ax1.set_zlabel('Implied Volatility')\n",
        "    ax1.set_title('IV Surface - 3D View')\n",
        "    ax1.view_init(elev=20, azim=45)\n",
        "    fig.colorbar(surf, ax=ax1, shrink=0.5)\n",
        "\n",
        "    # Heatmap\n",
        "    ax2 = fig.add_subplot(132)\n",
        "    im = ax2.imshow(iv_surface.cpu(), aspect='auto', cmap='viridis',\n",
        "                   extent=[k_range[0], k_range[1], T_range[0], T_range[1]],\n",
        "                   origin='lower')\n",
        "    ax2.set_xlabel('Log-Moneyness')\n",
        "    ax2.set_ylabel('Maturity (years)')\n",
        "    ax2.set_title('IV Surface - Heatmap')\n",
        "    fig.colorbar(im, ax=ax2)\n",
        "\n",
        "    # Smile slices at different maturities\n",
        "    ax3 = fig.add_subplot(133)\n",
        "    maturity_indices = [0, n_points//4, n_points//2, 3*n_points//4, n_points-1]\n",
        "    for idx in maturity_indices:\n",
        "        T_val = T_grid[idx].item()\n",
        "        ax3.plot(k_grid.cpu(), iv_surface[idx, :].cpu(),\n",
        "                label=f'T={T_val:.2f}y', linewidth=2)\n",
        "    ax3.set_xlabel('Log-Moneyness')\n",
        "    ax3.set_ylabel('Implied Volatility')\n",
        "    ax3.set_title('Volatility Smiles')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{CONFIG['output_dir']}/visualizations/full_surface.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"\\nSurface Statistics:\")\n",
        "    print(f\"  Shape: {iv_surface.shape}\")\n",
        "    print(f\"  IV range: [{iv_surface.min():.4f}, {iv_surface.max():.4f}]\")\n",
        "    print(f\"  Mean IV: {iv_surface.mean():.4f} ± {iv_surface.std():.4f}\")\n",
        "\n",
        "    # Parameters used (denormalized)\n",
        "    param_names = builder.process.param_info.names if hasattr(builder.process, 'param_info') else ['p'+str(i) for i in range(len(theta))]\n",
        "    print(f\"\\nParameters used:\")\n",
        "    for name, value in zip(param_names, theta.cpu().numpy()):\n",
        "        print(f\"  {name}: {value:.4f}\")\n",
        "\n",
        "    return iv_surface, T_mesh, k_mesh"
      ],
      "metadata": {
        "id": "kC3-StPj7dFC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# === MAIN EXECUTION ===\n",
        "# ===============================================================\n",
        "\n",
        "# Run the complete training\n",
        "if __name__ == \"__main__\":\n",
        "    # Run training\n",
        "    pricer, builder, history = run_complete_random_grids_training()\n",
        "\n",
        "    # Generate a complete surface as a final example\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"GENERATING FULL SURFACE EXAMPLE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Use example parameters\n",
        "    example_theta = builder.sample_theta_lhs(1)[0]\n",
        "    iv_surface, T_mesh, k_mesh = generate_full_surface(\n",
        "        pricer, builder, example_theta,\n",
        "        T_range=(0.05, 2.0),\n",
        "        k_range=(-0.4, 0.4),\n",
        "        n_points=40\n",
        "    )\n",
        "\n",
        "    print(\"\\n✓ COMPLETE EXECUTION FINISHED!\")\n",
        "    print(f\"All results saved to: {CONFIG['output_dir']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        },
        "id": "5jk3n6XR7gQA",
        "outputId": "b6a0ad6f-13dd-4ad6-a609-a41a5b90282b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            " RANDOM GRIDS POINTWISE NETWORK TRAINING \n",
            "============================================================\n",
            "\n",
            "Configuration:\n",
            "  Process: rough_heston\n",
            "  Device: cuda\n",
            "  Training surfaces: 7000\n",
            "  Validation smiles: 10000\n",
            "  Network: [30, 30, 30, 30]\n",
            "  Output: /content/drive/MyDrive/random_grids_results\n",
            "\n",
            "============================================================\n",
            "STEP 1: DATASET GENERATION\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "DATASET GENERATION WITH RANDOM GRIDS\n",
            "============================================================\n",
            "\n",
            "✔ Created process: RoughHestonProcess\n",
            "  Parameters: ['H', 'nu', 'rho', 'kappa', 'theta_var']\n",
            "  Supports absorption: True\n",
            "\n",
            ">>> Generating Training Dataset with Random Grids\n",
            "    Surfaces: 7000\n",
            "    Maturities per surface: 11\n",
            "    Strikes per maturity: 13\n",
            "    Total points: 1,001,000\n",
            "\n",
            "Building Random Grids Pointwise Dataset: 7000 × 11 × 13\n",
            "Starting from surface 0 [train]\n",
            "\n",
            "Building Random Grids Pointwise Dataset: 7000 × 11 × 13\n",
            "Starting from surface 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating random grids:   0%|          | 3/7000 [01:53<73:30:20, 37.82s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2980812100.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Esegui training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpricer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_complete_random_grids_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Genera una superficie completa come esempio finale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3109810797.py\u001b[0m in \u001b[0;36mrun_complete_random_grids_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mbuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Cleanup memoria dopo generazione dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-691469017.py\u001b[0m in \u001b[0;36mgenerate_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mt_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     theta_train, T_train, k_train, iv_train = train_builder.build_random_grids_dataset(\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mn_surfaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_surfaces\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mn_maturities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_maturities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deepLearningVolatility/deepLearningVolatility/nn/dataset_builder/dataset_builder.py\u001b[0m in \u001b[0;36mbuild_random_grids_dataset\u001b[0;34m(self, n_surfaces, n_maturities, n_strikes, n_paths, spot, normalize, compute_stats_from, show_progress, batch_size, checkpoint_every, resume_from, save_all_thetas, base_seed, split)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m             \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthetas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_random_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_maturities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_strikes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maturities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deepLearningVolatility/deepLearningVolatility/nn/dataset_builder/dataset_builder.py\u001b[0m in \u001b[0;36m_generate_random_grid\u001b[0;34m(self, theta, n_maturities, n_strikes_per_maturity, spot, mc_params)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0msmile_repair_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pchip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m             )\n\u001b[0;32m-> 1241\u001b[0;31m             iv_smile = pricer._mc_iv_grid(\n\u001b[0m\u001b[1;32m   1242\u001b[0m                 \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                 \u001b[0mn_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmc_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n_paths'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deepLearningVolatility/deepLearningVolatility/nn/pricer/pricer.py\u001b[0m in \u001b[0;36m_mc_iv_grid\u001b[0;34m(self, theta, n_paths, spot, price_floor, use_antithetic, adaptive_paths, adaptive_dt, control_variate, chunk_size, handle_absorption, q)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# Single simulation up to T_max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             sim_result = self.process.simulate(\n\u001b[0m\u001b[1;32m    779\u001b[0m                 \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m                 \u001b[0mn_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_chunk_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deepLearningVolatility/deepLearningVolatility/stochastic/wrappers/rough_heston_wrapper.py\u001b[0m in \u001b[0;36msimulate\u001b[0;34m(self, theta, n_paths, n_steps, dt, init_state, device, dtype, antithetic, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Simulate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mspot_variance_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_rough_heston\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msim_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;31m# Create output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deepLearningVolatility/deepLearningVolatility/stochastic/rough_heston.py\u001b[0m in \u001b[0;36mgenerate_rough_heston\u001b[0;34m(n_paths, n_steps, init_state, H, nu, rho, kappa, theta, dt, dtype, device, antithetic)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         u_shifted, chi_shifted = vectorized_qe_bivariate(\n\u001b[0m\u001b[1;32m    270\u001b[0m             \u001b[0mmean_shifted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_shifted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_chi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_u_chi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n",
            "\u001b[0;32m/content/deepLearningVolatility/deepLearningVolatility/stochastic/rough_heston.py\u001b[0m in \u001b[0;36mvectorized_qe_bivariate\u001b[0;34m(mean1, mean2, var1, var2, cov12, Z1, Z2)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \"\"\"\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# 1. Sample the first variable unconditionally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mval1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorized_qe_scheme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# 2. Sample the second variable from its conditional distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deepLearningVolatility/deepLearningVolatility/stochastic/rough_heston.py\u001b[0m in \u001b[0;36mvectorized_qe_scheme\u001b[0;34m(mean, var, Z)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask_quad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mpsi_quad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_quad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpsi_quad\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpsi_quad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpsi_quad\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_safe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_quad\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_quad\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_quad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0massigned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWRAPPER_ASSIGNMENTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massigned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}